{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('./OppScrData.xlsx', index_col=None)\n",
    "data=data[data.columns[0:52]]\n",
    "data.head()\n",
    "clinical_col=data.columns[0:14]\n",
    "outcome_col=data.columns[15:40]\n",
    "CT_col=data.columns[41:52]\n",
    "CT_data=data[CT_col]\n",
    "CT_data=CT_data.fillna(CT_data.mean())\n",
    "CT_data['Liver HU    (Median)']=pd.to_numeric(CT_data['Liver HU    (Median)'],errors='coerce')\n",
    "CT_data=CT_data.fillna(CT_data.mean())\n",
    "CT_data=CT_data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CL_data=data[clinical_col]\n",
    "CL_data=CL_data.drop(columns=['Record ID', 'Visit ID', 'PT ID','BMI >30', 'Met Sx'])\n",
    "CL_data['BMI']=CL_data['BMI'].fillna(CL_data['BMI'].mean())\n",
    "(CL_data['Tobacco']=='Yes').sum()/len(CL_data)\n",
    "CL_data['Tobacco']=CL_data['Tobacco'].fillna('No')\n",
    "CL_data['Alcohol abuse']=CL_data['Alcohol abuse'].fillna(0)\n",
    "CL_data['Alcohol abuse']=CL_data['Alcohol abuse'].apply(lambda x: 1 if x!=0 else 0)\n",
    "for i in ['Sex','Tobacco']:\n",
    "    CL_data[i] = pd.factorize(CL_data[i])[0] + 1\n",
    "CL_data['FRS 10-year risk (%)']=CL_data['FRS 10-year risk (%)'].apply(lambda x: 0.01 if x=='<1%' else x)\n",
    "CL_data['FRS 10-year risk (%)']=CL_data['FRS 10-year risk (%)'].apply(lambda x: 0.3 if x=='>30%' else x)\n",
    "CL_data['FRS 10-year risk (%)']=CL_data['FRS 10-year risk (%)'].apply(lambda x: 0 if x=='X' else x)\n",
    "for i in ['FRS 10-year risk (%)','FRAX 10y Fx Prob (Orange-w/ DXA)','FRAX 10y Hip Fx Prob (Orange-w/ DXA)']:\n",
    "    CL_data[i]=pd.to_numeric(CL_data[i],errors='coerce')\n",
    "    CL_data[i]=CL_data[i].fillna(CL_data[i].mean())\n",
    "CL_CT_data=pd.concat([CL_data,CT_data],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = CL_CT_data.values\n",
    "training_X, testing_X = train_test_split(X, test_size = 0.2)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "training_X = sc.fit_transform(training_X)\n",
    "testing_X = sc.transform(testing_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7378, 20)\n",
      "(1845, 20)\n"
     ]
    }
   ],
   "source": [
    "print (training_X.shape)\n",
    "print (testing_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 6\n",
    "\n",
    "class Autoencoder(Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.latent_dim = latent_dim   \n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Dense(16, activation='relu'),\n",
    "      layers.Dense(8, activation='tanh'),\n",
    "      layers.Dense(latent_dim, activation='relu'),\n",
    "    ])\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Dense(8, activation='tanh'),\n",
    "      layers.Dense(16, activation='relu'),\n",
    "      layers.Dense(np.size(X,1), activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "\n",
    "autoencoder = Autoencoder(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0394 - val_loss: 0.0225\n",
      "Epoch 2/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0150 - val_loss: 0.0124\n",
      "Epoch 3/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0104 - val_loss: 0.0092\n",
      "Epoch 4/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0083 - val_loss: 0.0077\n",
      "Epoch 5/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0063 - val_loss: 0.0058\n",
      "Epoch 6/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0052 - val_loss: 0.0054\n",
      "Epoch 7/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0049 - val_loss: 0.0052\n",
      "Epoch 8/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0047 - val_loss: 0.0050\n",
      "Epoch 9/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0046 - val_loss: 0.0049\n",
      "Epoch 10/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0045 - val_loss: 0.0047\n",
      "Epoch 11/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0040 - val_loss: 0.0038\n",
      "Epoch 12/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 13/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 14/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 15/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0027 - val_loss: 0.0028\n",
      "Epoch 16/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0025 - val_loss: 0.0027\n",
      "Epoch 17/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0024 - val_loss: 0.0026\n",
      "Epoch 18/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0023 - val_loss: 0.0025\n",
      "Epoch 19/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 20/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 21/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0021 - val_loss: 0.0023\n",
      "Epoch 22/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0021 - val_loss: 0.0023\n",
      "Epoch 23/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 24/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0020 - val_loss: 0.0022\n",
      "Epoch 25/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0020 - val_loss: 0.0022\n",
      "Epoch 26/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 27/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 28/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 29/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 30/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 31/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 32/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 33/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 34/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 35/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 36/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 37/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 38/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 39/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 40/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 41/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 42/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 43/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 44/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 45/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 46/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 47/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 48/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 49/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 50/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 51/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 52/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 53/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 54/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 55/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 56/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 57/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 58/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 59/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 60/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 61/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 62/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 63/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 64/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 65/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 66/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 67/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 68/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 69/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 70/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 71/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 72/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 73/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 74/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 75/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 76/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 77/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 78/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 79/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 80/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 81/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 82/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 83/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 84/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 85/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 86/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 87/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 88/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 89/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 90/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 91/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 92/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 93/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 94/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 95/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 96/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 97/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 98/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 99/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x243d87e4b80>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "autoencoder.fit(training_X, training_X,\n",
    "                epochs=200,\n",
    "                shuffle=True,\n",
    "                batch_size=16, \n",
    "                callbacks=[callback],\n",
    "                validation_data=(testing_X, testing_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank=6000\n",
    "data['DEATH [d from CT]']=data['DEATH [d from CT]'].fillna(blank)\n",
    "death=data['DEATH [d from CT]'][data['DEATH [d from CT]']!=blank]\n",
    "y = death.values\n",
    "CL_CT_data=CL_CT_data[data['DEATH [d from CT]']!=blank]\n",
    "X=CL_CT_data.values\n",
    "X=sc.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedX=autoencoder.encoder(X).numpy()\n",
    "X_train, X_test ,y_train,y_test= train_test_split(encodedX, y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(train_X,train_y,test_x,K):\n",
    "    dist=np.zeros(len(train_y))\n",
    "    for i in range(len(train_y)):\n",
    "        dist[i]=np.linalg.norm(test_x-train_X[i])\n",
    "    dist_ind=dist.argsort()[0:K]\n",
    "    y_lab=[]\n",
    "    for i in range(K):\n",
    "        y_lab.append(np.float64(train_y[dist_ind[i]]))\n",
    "    return(y_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "570.3240909090908"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=[]\n",
    "y_lab=[]\n",
    "acc=[]\n",
    "kk=20\n",
    "for i in range(len(y_test)):\n",
    "    y_pred_all=KNN(X_train,y_train,X_test[i],kk)\n",
    "    y_pred.append(np.mean(y_pred_all))\n",
    "    y_lab.append(y_pred[i])\n",
    "    acc.append(y_lab[i]-y_test[i])\n",
    "np.absolute(acc).sum()/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[161.70000000000005]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.quantile(acc,0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x243da6b92b0>,\n",
       "  <matplotlib.lines.Line2D at 0x243da6b9610>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x243da6b9970>,\n",
       "  <matplotlib.lines.Line2D at 0x243da6b9cd0>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x243dbb1bf10>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x243da6c9070>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x243da6c9370>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(np.absolute(acc))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13f457fc9c733174e4480ab93c31bb27af2163be8665e63275895a21472bd25c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
