{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('./OppScrData.xlsx', index_col=None)\n",
    "data=data[data.columns[0:52]]\n",
    "data.head()\n",
    "clinical_col=data.columns[0:14]\n",
    "outcome_col=data.columns[15:40]\n",
    "CT_col=data.columns[41:52]\n",
    "CT_data=data[CT_col]\n",
    "CT_data=CT_data.fillna(CT_data.mean())\n",
    "CT_data['Liver HU    (Median)']=pd.to_numeric(CT_data['Liver HU    (Median)'],errors='coerce')\n",
    "CT_data=CT_data.fillna(CT_data.mean())\n",
    "CT_data=CT_data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "CL_data=data[clinical_col]\n",
    "CL_data=CL_data.drop(columns=['Record ID', 'Visit ID', 'PT ID','BMI >30', 'Met Sx'])\n",
    "CL_data['BMI']=CL_data['BMI'].fillna(CL_data['BMI'].mean())\n",
    "(CL_data['Tobacco']=='Yes').sum()/len(CL_data)\n",
    "CL_data['Tobacco']=CL_data['Tobacco'].fillna('No')\n",
    "CL_data['Alcohol abuse']=CL_data['Alcohol abuse'].fillna(0)\n",
    "CL_data['Alcohol abuse']=CL_data['Alcohol abuse'].apply(lambda x: 1 if x!=0 else 0)\n",
    "for i in ['Sex','Tobacco']:\n",
    "    CL_data[i] = pd.factorize(CL_data[i])[0] + 1\n",
    "CL_data['FRS 10-year risk (%)']=CL_data['FRS 10-year risk (%)'].apply(lambda x: 0.005 if x=='<1%' else x)\n",
    "CL_data['FRS 10-year risk (%)']=CL_data['FRS 10-year risk (%)'].apply(lambda x: 0.3 if x=='>30%' else x)\n",
    "fm=np.mean(CL_data['FRS 10-year risk (%)'][CL_data['FRS 10-year risk (%)']!='X'])\n",
    "CL_data['FRS 10-year risk (%)']=CL_data['FRS 10-year risk (%)'].apply(lambda x: fm if x=='X' else x)\n",
    "for i in ['FRS 10-year risk (%)','FRAX 10y Fx Prob (Orange-w/ DXA)','FRAX 10y Hip Fx Prob (Orange-w/ DXA)']:\n",
    "    CL_data[i]=pd.to_numeric(CL_data[i],errors='coerce')\n",
    "    CL_data[i]=CL_data[i].fillna(CL_data[i].mean())\n",
    "CL_CT_data=pd.concat([CL_data,CT_data],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = CL_CT_data.values\n",
    "training_X, testing_X = train_test_split(X, test_size = 0.2)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "training_X = sc.fit_transform(training_X)\n",
    "testing_X = sc.transform(testing_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7378, 20)\n",
      "(1845, 20)\n"
     ]
    }
   ],
   "source": [
    "print (training_X.shape)\n",
    "print (testing_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 6\n",
    "\n",
    "class Autoencoder(Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.latent_dim = latent_dim   \n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Dense(16, activation='relu'),\n",
    "      layers.Dense(8, activation='tanh'),\n",
    "      layers.Dense(latent_dim, activation='relu'),\n",
    "    ])\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Dense(8, activation='tanh'),\n",
    "      layers.Dense(16, activation='relu'),\n",
    "      layers.Dense(np.size(X,1), activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "\n",
    "autoencoder = Autoencoder(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0439 - val_loss: 0.0171\n",
      "Epoch 2/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0133 - val_loss: 0.0123\n",
      "Epoch 3/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0111 - val_loss: 0.0099\n",
      "Epoch 4/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0081 - val_loss: 0.0071\n",
      "Epoch 5/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0059 - val_loss: 0.0057\n",
      "Epoch 6/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0051 - val_loss: 0.0052\n",
      "Epoch 7/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0048 - val_loss: 0.0050\n",
      "Epoch 8/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0047 - val_loss: 0.0049\n",
      "Epoch 9/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0046 - val_loss: 0.0049\n",
      "Epoch 10/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0045 - val_loss: 0.0048\n",
      "Epoch 11/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0045 - val_loss: 0.0047\n",
      "Epoch 12/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0044 - val_loss: 0.0045\n",
      "Epoch 13/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0042 - val_loss: 0.0043\n",
      "Epoch 14/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 15/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0037 - val_loss: 0.0038\n",
      "Epoch 16/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0036 - val_loss: 0.0037\n",
      "Epoch 17/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0036 - val_loss: 0.0037\n",
      "Epoch 18/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0035 - val_loss: 0.0037\n",
      "Epoch 19/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 20/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0035 - val_loss: 0.0037\n",
      "Epoch 21/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 22/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 23/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 24/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 25/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0034 - val_loss: 0.0036\n",
      "Epoch 26/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 27/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0033 - val_loss: 0.0035\n",
      "Epoch 28/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 29/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 30/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0025 - val_loss: 0.0027\n",
      "Epoch 31/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0023 - val_loss: 0.0026\n",
      "Epoch 32/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0022 - val_loss: 0.0025\n",
      "Epoch 33/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0022 - val_loss: 0.0025\n",
      "Epoch 34/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0021 - val_loss: 0.0025\n",
      "Epoch 35/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0021 - val_loss: 0.0024\n",
      "Epoch 36/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0021 - val_loss: 0.0024\n",
      "Epoch 37/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0021 - val_loss: 0.0024\n",
      "Epoch 38/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0021 - val_loss: 0.0024\n",
      "Epoch 39/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 40/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 41/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 42/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 43/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0019 - val_loss: 0.0023\n",
      "Epoch 44/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0019 - val_loss: 0.0023\n",
      "Epoch 45/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 46/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 47/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 48/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 49/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 50/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 51/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 52/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 53/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 54/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 55/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 56/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 57/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 58/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 59/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 60/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 61/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 62/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 63/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 64/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 65/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 66/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 67/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 68/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 69/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 70/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 71/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 72/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 73/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 74/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 75/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 76/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 77/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 78/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 79/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 80/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 81/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 82/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 83/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 84/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 85/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 86/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 87/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 88/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 89/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 90/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 91/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 92/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 93/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 94/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 95/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 96/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 97/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 98/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 99/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 100/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 101/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 102/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 103/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 104/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 105/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 106/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 107/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 108/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 109/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 110/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 111/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 112/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 113/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 114/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 115/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 116/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 117/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 118/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 119/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 120/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 121/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 122/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 123/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 124/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 125/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 126/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 127/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 128/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 129/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 130/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 131/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 132/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 133/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 134/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 135/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 136/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 137/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 138/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 139/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 140/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 141/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 142/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 143/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 144/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 145/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 146/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 147/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 148/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 149/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 150/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 151/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 152/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 153/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 154/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 155/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 156/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 157/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 158/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 159/200\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 160/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 161/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 162/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 163/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 164/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 165/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 166/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 167/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 168/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 169/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 170/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 171/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 172/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 173/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 174/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 175/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 176/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 177/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 178/200\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x243d8ce3280>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "autoencoder.fit(training_X, training_X,\n",
    "                epochs=200,\n",
    "                shuffle=True,\n",
    "                batch_size=16, \n",
    "                callbacks=[callback],\n",
    "                validation_data=(testing_X, testing_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-97-05a2f86b0dfb>:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  CL_CT_data=CL_CT_data[data['DEATH [d from CT]']!=blank]\n"
     ]
    }
   ],
   "source": [
    "blank=6000\n",
    "data['DEATH [d from CT]']=data['DEATH [d from CT]'].fillna(blank)\n",
    "death=data['DEATH [d from CT]'][data['DEATH [d from CT]']!=blank]\n",
    "y = death.values\n",
    "CL_CT_data=CL_CT_data[data['DEATH [d from CT]']!=blank]\n",
    "X=CL_CT_data.values\n",
    "X=sc.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(549,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedX=autoencoder.encoder(X).numpy()\n",
    "X_train, X_test ,y_train,y_test= train_test_split(encodedX, y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(train_X,train_y,test_x,K):\n",
    "    dist=np.zeros(len(train_y))\n",
    "    for i in range(len(train_y)):\n",
    "        dist[i]=np.linalg.norm(test_x-train_X[i])\n",
    "    dist_ind=dist.argsort()[0:K]\n",
    "    y_lab=[]\n",
    "    for i in range(K):\n",
    "        y_lab.append(np.float64(train_y[dist_ind[i]]))\n",
    "    return(y_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509.7836363636362"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=[]\n",
    "y_lab=[]\n",
    "acc=[]\n",
    "kk=20\n",
    "for i in range(len(y_test)):\n",
    "    y_pred_all=KNN(X_train,y_train,X_test[i],kk)\n",
    "    y_pred.append(np.mean(y_pred_all))\n",
    "    y_lab.append(y_pred[i])\n",
    "    acc.append(y_lab[i]-y_test[i])\n",
    "np.absolute(acc).sum()/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x243d8bfa8b0>,\n",
       "  <matplotlib.lines.Line2D at 0x243d8bfac10>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x243d8bfaf70>,\n",
       "  <matplotlib.lines.Line2D at 0x243d8c8e310>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x243d8bfa550>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x243d8c8e670>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x243d8c8e970>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "CL_CT_data=pd.concat([CL_data,CT_data],axis=1)\n",
    "CL_CT_data_na=CL_CT_data[data['DEATH [d from CT]']==blank]\n",
    "X_na=CL_CT_data.values\n",
    "X_na=sc.transform(X_na)\n",
    "encodedX_na=autoencoder.encoder(X_na).numpy()\n",
    "y_pred_na=[]\n",
    "y_lab_na=[]\n",
    "acc_na=[]\n",
    "kk=20\n",
    "for i in range(np.shape(encodedX_na)[0]):\n",
    "    y_pred_all=KNN(encodedX,y,encodedX_na[i],kk)\n",
    "    y_pred_na.append(np.mean(y_pred_all))\n",
    "    y_lab_na.append(y_pred_na[i])\n",
    "    acc_na.append(y_lab_na[i]-X_na[i,0])\n",
    "plt.boxplot(acc_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[66.57499999999993]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.quantile(acc,0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x243e5c2e7c0>,\n",
       "  <matplotlib.lines.Line2D at 0x243e5c2eb20>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x243e5c2ee80>,\n",
       "  <matplotlib.lines.Line2D at 0x243e5c24220>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x243e5c2e460>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x243e5c24580>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x243e5c24880>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(np.absolute(acc))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13f457fc9c733174e4480ab93c31bb27af2163be8665e63275895a21472bd25c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
